{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:6666/ (Press CTRL+C to quit)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Student\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.793 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\base.py:154: UserWarning: Loky-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  n_jobs = min(effective_n_jobs(n_jobs), n_estimators)\n",
      "127.0.0.1 - - [25/Jun/2019 02:23:16] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import jieba\n",
    "import time\n",
    "import jieba.analyse\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import heapq\n",
    "from sklearn.externals import joblib\n",
    "from flask import Flask, jsonify, request\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get Parameters to DataFrame    \n",
    "    json_ = request.form.to_dict()\n",
    "    # Pridicting\n",
    "    name_test = ['{}'.format(json_['text'])]\n",
    "    name_test = pd.DataFrame(name_test)\n",
    "    name_test = name_test.rename(columns={0: 'name'})\n",
    "    name_test['content_cutted'] = name_test['name'].apply(word_cut)\n",
    "    global code_Tfidf\n",
    "    code_Tfidf = Data_prepara()\n",
    "    with open('dictionary.pickle', 'rb') as handle:\n",
    "        dictionary = pickle.load(handle)\n",
    "    content_cutted = pd.Series([dictionary.doc2bow(x) for x in name_test['content_cutted']])\n",
    "    content_cutted2 = pd.Series(map( lambda x: list(map( lambda y: y[0] ,x)),content_cutted))\n",
    "    testX = model(content_cutted2)\n",
    "    test_pred_label = clf.predict_proba(testX)\n",
    "    google_id_dict = dic()\n",
    "    max_num_index_list = list(map(test_pred_label[0].tolist().index, heapq.nlargest(5, test_pred_label[0].tolist())))\n",
    "    d = dict()\n",
    "    for i in range(0,5):\n",
    "        d[i+1] = '標籤:'+google_id_dict[str(clf.classes_[max_num_index_list[i]])]+',權重:'+str(test_pred_label[0][max_num_index_list[i]])\n",
    "    return jsonify(d)\n",
    "            \n",
    "def word_cut(mytext):\n",
    "    return list(jieba.cut(mytext))\n",
    "def dic():\n",
    "    with open('google-id-dict2.pickle', 'rb') as handle:\n",
    "        google_id_dict = pickle.load(handle)\n",
    "    return google_id_dict\n",
    "def Data_prepara():\n",
    "    csv.field_size_limit(int(sys.maxsize/10000000000))\n",
    "#     df2=pd.read_csv(r'C:\\Mypython\\專題\\Join-name5(周周版).csv', sep=',', engine='python',encoding='utf_8_sig')\n",
    "#     df2['content_cutted'] = df2['Join_name'].astype('str').apply(word_cut)\n",
    "#     df_Join_name_count = pd.Series([Counter(x) for x in df2['content_cutted']],index=df2['Join_name'].index)\n",
    "#     global dictionary\n",
    "#     dictionary = Dictionary(list(df_Join_name_count))\n",
    "#     code_tokenGybow = pd.Series([dictionary.doc2bow(x) for x in df_Join_name_count],index=df2['Join_name'].index)\n",
    "#     tfidf = TfidfModel(code_tokenGybow)\n",
    "#     code_tokenGyTfidf = pd.Series(tfidf[code_tokenGybow],index=df2['Join_name'].index)\n",
    "#     code_tokenGyTfidfSort = pd.Series(map( lambda x: sorted(x,key=lambda w: w[1],reverse=True),code_tokenGyTfidf),\n",
    "#                                       index=df2['Join_name'].index)\n",
    "#     code_tokenGyTfidfSort2 = pd.Series(map( lambda x: x[:len(x)//2],code_tokenGyTfidfSort),\n",
    "#                                        index=df2['Join_name'].index)\n",
    "#     code_tokenGyTfidfSort3 = pd.Series(map( lambda x: list(map( lambda y: y[0]  ,x)),\n",
    "#                                        code_tokenGyTfidfSort),index=df2['Join_name'].index)\n",
    "    df2=pd.read_csv('特徵資料.csv', engine='python',encoding='utf_8_sig')\n",
    "    df3 = df2['tokens'].apply(lambda x :x.replace('[','').replace(']','').split(','))\n",
    "    code_tokenGyTfidfSort4 =list(map( lambda x : list(map(toInt,x)) ,df3))\n",
    "    return code_tokenGyTfidfSort4\n",
    "def toInt(x):\n",
    "    return int(x.strip())\n",
    "def word(x):\n",
    "    stroke = []\n",
    "    for i in range(0,len(code_Tfidf)):    \n",
    "        stroke.append(len(set(x)&set(code_Tfidf[i])))\n",
    "    return(stroke)\n",
    "def model(x):\n",
    "    AA = []\n",
    "    AA.append(list(x.apply(word)))\n",
    "    df_AA = pd.DataFrame(AA[0])\n",
    "    return df_AA\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"127.0.0.1\",port=6666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "clf = joblib.load(\"分類RandomForest_train_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
